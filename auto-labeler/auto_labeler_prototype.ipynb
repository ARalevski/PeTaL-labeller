{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "auto-labeler-prototype.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOcH8325KA2q47Ob+czXq+n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nasa/PeTaL/blob/labeling-text-classification/auto-labeler/auto_labeler_prototype.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft-7aQbboP7f"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpdAH3eLn9KA"
      },
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9vxghfPocM5"
      },
      "source": [
        "# GPU detection \n",
        "\n",
        "# Get GPU device name\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGAOqbInomgX"
      },
      "source": [
        "# If there is a GPU available\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use GPU\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3kzChKspSad"
      },
      "source": [
        "# Import data (into Pandas dataframe?) and parse\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# Split into `docs` and `labels`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ21pfQvgtNA"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load BERT tokenizer\n",
        "print('Loading BERT tokenizer')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTUSIIZAhcVT"
      },
      "source": [
        "# Make sure it is tokenizing correctly:\n",
        "\n",
        "# Print original articles\n",
        "print(' Original: ', docs[0])\n",
        "\n",
        "# Print a doc split into tokens\n",
        "print('Tokenized: ', tokenizer.tokenize(docs[0]))\n",
        "\n",
        "# Print docs as mapped to ids\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(docs[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9mSOV7GhvdA"
      },
      "source": [
        "max_len = 0\n",
        "\n",
        "for d in doc:\n",
        "\n",
        "    # Tokenize text and add `[CLS]` and `[SEP]` tokens\n",
        "    input_ids = tokenizer.encode(d, add_special_tokens=True)\n",
        "\n",
        "    # Update max length\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgtsq-3s0jyF"
      },
      "source": [
        "# Finishing tokenizing all docs and map tokens to thier word IDs\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for d in docs:\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        d,                      # Docs to encode.\n",
        "                        truncation=True,\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 256,           # Pad & truncate all docs\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Attention masks\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', docs[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "print('Reverse:', tokenizer.convert_ids_to_tokens(input_ids[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVofTi9U01pb"
      },
      "source": [
        "# Split up training & testing/validation\n",
        "\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# 90:10 split\n",
        "\n",
        "# Number of samples to include per set\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceVT2Jmoph4x"
      },
      "source": [
        "## Ignore this cell for now\n",
        "# Trying out example BERT\n",
        "\n",
        "# Single training/test example for simple sequence classification\n",
        "class InputExample(object):\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "            label: (Optional) string. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"Single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}