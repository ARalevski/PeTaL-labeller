{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "auto-labeler-prototype.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e33f5caa4f3a4c189159106e4c4a9ec0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_063cd89dee8f487c9053e6ead3aba09e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_318bc872f5f444188a4391a9a16cf661",
              "IPY_MODEL_8bc7df069b1e492bb818904e393dc488"
            ]
          }
        },
        "063cd89dee8f487c9053e6ead3aba09e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "318bc872f5f444188a4391a9a16cf661": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_db623ec1d1804a39b7d29e517d5302f1",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_22d8913a90c14eccb25b7d3af6c5a102"
          }
        },
        "8bc7df069b1e492bb818904e393dc488": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_096ddbc1c5444a36afc8eb06538a25c2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 1.97MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_756cc5bbb6d44dc182ff3d2748a4815e"
          }
        },
        "db623ec1d1804a39b7d29e517d5302f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "22d8913a90c14eccb25b7d3af6c5a102": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "096ddbc1c5444a36afc8eb06538a25c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "756cc5bbb6d44dc182ff3d2748a4815e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nasa/PeTaL-labeller/blob/SJ/auto-labeler/auto_labeler_prototype.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft-7aQbboP7f",
        "outputId": "04bf6744-b510-4164-fa26-bcd7aaea0da7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install tensorboardX\n",
        "!pip install wikipedia\n",
        "!pip install swifter"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.3.1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.0)\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: swifter in /usr/local/lib/python3.6/dist-packages (1.0.6)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0cloudpickle>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from swifter) (7.5.1)\n",
            "Requirement already satisfied: modin[ray]>=0.7.4 in /usr/local/lib/python3.6/dist-packages (from swifter) (0.8.1.1)\n",
            "Requirement already satisfied: bleach>=3.1.1 in /usr/local/lib/python3.6/dist-packages (from swifter) (3.2.1)\n",
            "Requirement already satisfied: psutil>=5.6.6 in /usr/local/lib/python3.6/dist-packages (from swifter) (5.7.2)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from swifter) (1.1.2)\n",
            "Requirement already satisfied: dask[dataframe]>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from swifter) (2.12.0)\n",
            "Requirement already satisfied: tqdm>=4.33.0 in /usr/local/lib/python3.6/dist-packages (from swifter) (4.41.1)\n",
            "Requirement already satisfied: parso>0.4.0 in /usr/local/lib/python3.6/dist-packages (from swifter) (0.7.1)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (5.5.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (4.10.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (3.5.1)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (4.3.3)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (5.0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from modin[ray]>=0.7.4->swifter) (20.4)\n",
            "Requirement already satisfied: pyarrow<0.17; extra == \"ray\" in /usr/local/lib/python3.6/dist-packages (from modin[ray]>=0.7.4->swifter) (0.14.1)\n",
            "Requirement already satisfied: ray>=1.0.0; extra == \"ray\" in /usr/local/lib/python3.6/dist-packages (from modin[ray]>=0.7.4->swifter) (1.0.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bleach>=3.1.1->swifter) (1.15.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach>=3.1.1->swifter) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.0->swifter) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.0->swifter) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.0->swifter) (1.18.5)\n",
            "Requirement already satisfied: toolz>=0.7.3; extra == \"dataframe\" in /usr/local/lib/python3.6/dist-packages (from dask[dataframe]>=2.10.0->swifter) (0.11.1)\n",
            "Requirement already satisfied: partd>=0.3.10; extra == \"dataframe\" in /usr/local/lib/python3.6/dist-packages (from dask[dataframe]>=2.10.0->swifter) (1.1.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0; extra == \"dataframe\" in /usr/local/lib/python3.6/dist-packages (from dask[dataframe]>=2.10.0->swifter) (0.8.3)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (50.3.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (4.4.2)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (5.3.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (5.3.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.3.1->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (4.6.3)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (2.6.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->modin[ray]>=0.7.4->swifter) (2.4.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (3.0.12)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (1.0.0)\n",
            "Requirement already satisfied: aioredis in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (1.3.1)\n",
            "Requirement already satisfied: aiohttp-cors in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (0.7.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (3.6.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (0.4.3)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (0.3.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (2.23.0)\n",
            "Requirement already satisfied: redis<3.5.0,>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (3.4.1)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (0.8.0)\n",
            "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (1.32.0)\n",
            "Requirement already satisfied: opencensus in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (0.7.10)\n",
            "Requirement already satisfied: google in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (2.0.3)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (3.12.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (7.1.2)\n",
            "Requirement already satisfied: colorful in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (0.5.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (3.13)\n",
            "Requirement already satisfied: gpustat in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (0.6.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.6/dist-packages (from partd>=0.3.10; extra == \"dataframe\"->dask[dataframe]>=2.10.0->swifter) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.2.5)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (19.0.2)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.9.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (1.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (2.11.2)\n",
            "Requirement already satisfied: async-timeout in /usr/local/lib/python3.6/dist-packages (from aioredis->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (3.0.1)\n",
            "Requirement already satisfied: hiredis in /usr/local/lib/python3.6/dist-packages (from aioredis->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (1.1.0)\n",
            "Requirement already satisfied: multidict<5.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (4.7.6)\n",
            "Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (1.1.0)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (3.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (1.6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (20.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (1.24.3)\n",
            "Requirement already satisfied: opencensus-context==0.1.1 in /usr/local/lib/python3.6/dist-packages (from opencensus->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (0.1.1)\n",
            "Requirement already satisfied: google-api-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from opencensus->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from google->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (4.6.3)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from gpustat->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (7.352.0)\n",
            "Requirement already satisfied: blessings>=1.6 in /usr/local/lib/python3.6/dist-packages (from gpustat->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (1.7)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.6.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.4.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (1.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (1.1.1)\n",
            "Requirement already satisfied: contextvars; python_version >= \"3.6\" and python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from opencensus-context==0.1.1->opencensus->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (2.4)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (1.17.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (1.52.0)\n",
            "Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars; python_version >= \"3.6\" and python_version < \"3.7\"->opencensus-context==0.1.1->opencensus->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (0.14)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (4.6)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.7.4->swifter) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpdAH3eLn9KA"
      },
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import wikipedia \n",
        "import swifter\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe0Z6A0rNUgf"
      },
      "source": [
        "## GPU Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9vxghfPocM5",
        "outputId": "54f85ba6-427e-4114-ec5b-c38ffc067966",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# GPU detection \n",
        "\n",
        "# Get GPU device name\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGAOqbInomgX",
        "outputId": "dbfdaac8-7a10-4b58-fc5d-ee91c7309aab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# If there is a GPU available\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use GPU\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-btVsbT0WETf"
      },
      "source": [
        "## Import, Parse, and Store Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz8CT3qvovyy"
      },
      "source": [
        "#Creating PyDrive instance to load in data from PeTaL shared drive, follow the steps to authenticate\n",
        "!pip install -U -q PyDrive \n",
        "  \n",
        "from pydrive.auth import GoogleAuth \n",
        "from pydrive.drive import GoogleDrive \n",
        "from google.colab import auth \n",
        "from oauth2client.client import GoogleCredentials \n",
        "  \n",
        "  \n",
        "# Authenticate and create the PyDrive client. \n",
        "auth.authenticate_user() \n",
        "gauth = GoogleAuth() \n",
        "gauth.credentials = GoogleCredentials.get_application_default() \n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5MO_gJnov1A"
      },
      "source": [
        "\n",
        "\n",
        "#this is the un-parsed articles\n",
        "# link = 'https://drive.google.com/file/d/1iIZgKs1swHHJuumCU5xyW8tXSAnKAg18/view?usp=sharing'\n",
        "# id = link.split(\"/\")[-2] \n",
        "  \n",
        "# downloaded = drive.CreateFile({'id':id})  \n",
        "# downloaded.GetContentFile('articles.csv')   \n",
        "#df = pd.read_csv('articles.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yveU_TvB7ZkD"
      },
      "source": [
        "#'https://petscan.wmflabs.org/' link to pull wikipedia articles and their page ID's"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCTYZEDsov41"
      },
      "source": [
        "#Scraping article content by ID\n",
        "def wiki_content(row):\n",
        "  id = row['pageid']\n",
        "  try:\n",
        "    content = wikipedia.page(pageid=id).content\n",
        "  except:\n",
        "    content = 'error'\n",
        "  return content\n",
        "\n",
        "df['Content'] = df.swifter.apply(wiki_content, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryadSHM_GdAO"
      },
      "source": [
        "#Scraping article summary by ID\n",
        "\n",
        "def wiki_summary(row):\n",
        "  id = row['pageid']\n",
        "  try:\n",
        "    summary = wikipedia.page(pageid=id).summary\n",
        "  except:\n",
        "    summary = 'error'\n",
        "  return summary\n",
        "\n",
        "df['Summary'] = df.swifter.apply(wiki_summary, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSf2JM6Ub7lI"
      },
      "source": [
        "#Saving parsed articles as csv, can be accessed in the \"Files\" folder on the left, then download if you want\n",
        "df.to_csv('parsed_articles.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-mVCjGb-nsw"
      },
      "source": [
        "#Google drive link to the parsed articles\n",
        "link = 'https://drive.google.com/file/d/1XRWsEsNUHjWOjPavwrfuUpaq3DwGGE4D/view?usp=sharing'\n",
        "id = link.split(\"/\")[-2] \n",
        " \n",
        "downloaded = drive.CreateFile({'id':id})  \n",
        "downloaded.GetContentFile('parsed_articles.csv') \n",
        "df = pd.read_csv('parsed_articles.csv')\n",
        "\n",
        "df = df[(df['Content'] != 'error') & df['Content'].notnull()]\n",
        "\n",
        "#Df 'Content' column into list\n",
        "docs = list(df['Content'].values)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXiUqqPwBZfB"
      },
      "source": [
        "#Labels\n",
        "\n",
        "labels = ['Maintain homeostasis', 'Protect from temperature']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnZWpxgW07DY"
      },
      "source": [
        "df['Content'].value_counts().to_frame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ktg_q4es2Oz3"
      },
      "source": [
        "labeled_df_link = 'https://drive.google.com/file/d/1MJDIPe1C0dFHIPWu0w18IEJhVk4Xbk2x/view?usp=sharing'\n",
        "#'https://drive.google.com/file/d/1OZnAk64SPXfnaEzQFfhDJd6AX3dntIy9/view?usp=sharing'\n",
        "labeled_id = labeled_df_link.split(\"/\")[-2]\n",
        "labeled_downloaded = drive.CreateFile({'id':labeled_id})  \n",
        "labeled_downloaded.GetContentFile('single_label.csv') \n",
        "#'Biological-Strategies-Export-2020-October-01-1849 (1).csv'\n",
        "labeled_df = pd.read_csv('single_label.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll5Y5I3C2Ske"
      },
      "source": [
        "labeled_df = labeled_df[['id', 'Title', 'Living Systems', 'Sources_source_link', 'Functions', 'Wikipedia', 'pdf_links', 'single_label']]\n",
        "labeled_df = labeled_df[labeled_df['Functions'].notnull( )]\n",
        "labeled_df = labeled_df[labeled_df['Sources_source_link'].notnull()]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3yE-cvH2Upv",
        "outputId": "ca453b86-2c54-4331-a05d-0ac4116cc8ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import urllib.request\n",
        "!pip install pdfminer\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfdocument import PDFDocument\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from pdfminer.pdfparser import PDFParser\n",
        "from io import StringIO"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pdfminer in /usr/local/lib/python3.6/dist-packages (20191125)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.6/dist-packages (from pdfminer) (3.9.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdR50dTb2Xa1"
      },
      "source": [
        "#convert pdf into text corpus\n",
        "def convert_pdf_to_string(file_path):\n",
        "  output_string = StringIO()\n",
        "  with open(file_path, 'rb') as in_file:\n",
        "    parser = PDFParser(in_file)\n",
        "    doc = PDFDocument(parser)\n",
        "    rsrcmgr = PDFResourceManager()\n",
        "    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
        "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "    for page in PDFPage.create_pages(doc):\n",
        "      interpreter.process_page(page)\n",
        "\n",
        "  return (output_string.getvalue())\n",
        "\n",
        "#Parsing into text\n",
        "def parse_text(row):\n",
        "  link = row['Sources_source_link']\n",
        "  try:\n",
        "      response = urllib.request.urlopen(link)\n",
        "      file = open('doc.pdf', 'wb')\n",
        "      file.write(response.read())\n",
        "      file.close()\n",
        "      corpus = convert_pdf_to_string('doc.pdf')\n",
        "  except:\n",
        "      corpus = 'Web error occurred'\n",
        "  \n",
        "  return corpus"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O01roYkt2Zqb"
      },
      "source": [
        "#pdf_links = labeled_df[labeled_df['Sources_source_link'].str.endswith('.pdf')]\n",
        "#pdf_links['Text'] = pdf_links.apply(parse_text, axis=1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYydK3S02bsy",
        "outputId": "22254b77-dfa4-4eb9-ee5b-0e83721f044a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# pdf_links\n",
        "labeled_df"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Title</th>\n",
              "      <th>Living Systems</th>\n",
              "      <th>Sources_source_link</th>\n",
              "      <th>Functions</th>\n",
              "      <th>Wikipedia</th>\n",
              "      <th>pdf_links</th>\n",
              "      <th>single_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2324</td>\n",
              "      <td>Beak design absorbs high-energy impacts</td>\n",
              "      <td>Ramphastos toco</td>\n",
              "      <td>http://dx.doi.org/10.1016/j.actamat.2005.04.04...</td>\n",
              "      <td>Manage impact</td>\n",
              "      <td>The toco toucan (Ramphastos toco), also known ...</td>\n",
              "      <td>[]</td>\n",
              "      <td>Manage impact</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2362</td>\n",
              "      <td>Saliva regulates digestion</td>\n",
              "      <td>Heloderma suspectum</td>\n",
              "      <td>http://www.jbc.org/content/267/11/7402.abstract</td>\n",
              "      <td>Maintain homeostasis|Regulate cellular processes</td>\n",
              "      <td>The Gila monster (Heloderma suspectum,  HEE-lə...</td>\n",
              "      <td>[]</td>\n",
              "      <td>Maintain homeostasis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2367</td>\n",
              "      <td>'Bombs' distract predators</td>\n",
              "      <td>Swima bombiviridis</td>\n",
              "      <td>http://dx.doi.org/10.1126/science.1172488</td>\n",
              "      <td>Transform radiant energy (light)|Send light si...</td>\n",
              "      <td>Swima bombiviridis is a worm species that live...</td>\n",
              "      <td>['/content/325/5943.toc.pdf', '/content/sci/32...</td>\n",
              "      <td>Transform radiant energy (light)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2393</td>\n",
              "      <td>Organ generates electricity</td>\n",
              "      <td>Electrophorus electricus</td>\n",
              "      <td>https://epub.uni-regensburg.de/2108/</td>\n",
              "      <td>Modify electric charge|Transform electrical en...</td>\n",
              "      <td>The electric eel (Electrophorus electricus, ot...</td>\n",
              "      <td>['http://www.uni-regensburg.de/publikationen/m...</td>\n",
              "      <td>Modify electric charge</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2400</td>\n",
              "      <td>Wings generate lift</td>\n",
              "      <td>Apis mellifera</td>\n",
              "      <td>http://biomimetic.pbworks.com/f/Short-amplitud...</td>\n",
              "      <td>Move in/through gases</td>\n",
              "      <td>The western honey bee or European honey bee (A...</td>\n",
              "      <td>[]</td>\n",
              "      <td>Move in/through gases</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>93260</td>\n",
              "      <td>Pheromones turn nematodes into pest-killing ma...</td>\n",
              "      <td>Nematoda</td>\n",
              "      <td>https://biblio.ugent.be/publication/1269676/fi...</td>\n",
              "      <td>Capture, absorb, or filter organisms|Cooperate...</td>\n",
              "      <td>The nematodes (UK:  NEM-ə-tohdz, US:  NEEM- Gr...</td>\n",
              "      <td>['/articles/s41598-020-62817-y.pdf', '/article...</td>\n",
              "      <td>Capture, absorb, or filter organisms</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>93283</td>\n",
              "      <td>Interaction with adults leads to faster nest b...</td>\n",
              "      <td>Taeniopygia guttata</td>\n",
              "      <td>https://academic.oup.com/beheco/article/31/4/8...</td>\n",
              "      <td>Physically assemble structure|Self-replicate</td>\n",
              "      <td>The zebra finch (Taeniopygia guttata) is the m...</td>\n",
              "      <td>request error</td>\n",
              "      <td>Physically assemble structure</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>343</th>\n",
              "      <td>93287</td>\n",
              "      <td>Chemicals in oregano act as fungicide</td>\n",
              "      <td>Origanum vulgare</td>\n",
              "      <td>https://sci-hub.st/10.1111/1750-3841.12700,htt...</td>\n",
              "      <td>Chemically break down organic compounds|Distri...</td>\n",
              "      <td>Oregano (US: , UK: ; Origanum vulgare) is a fl...</td>\n",
              "      <td>[]</td>\n",
              "      <td>Chemically break down organic compounds</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>344</th>\n",
              "      <td>93338</td>\n",
              "      <td>Brain acts as both teacher and student</td>\n",
              "      <td>Taeniopygia guttata</td>\n",
              "      <td>https://elifesciences.org/articles/20944,https...</td>\n",
              "      <td>Encode/Decode|Learn|Differentiate signal from ...</td>\n",
              "      <td>The zebra finch (Taeniopygia guttata) is the m...</td>\n",
              "      <td>[]</td>\n",
              "      <td>Encode/Decode</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345</th>\n",
              "      <td>93416</td>\n",
              "      <td>Birds build responsively</td>\n",
              "      <td>Taeniopygia guttata, zebra finch</td>\n",
              "      <td>https://sci-hub.st/10.1098/rspb.2013.3225,http...</td>\n",
              "      <td>Physically assemble structure|Adapt behaviors|...</td>\n",
              "      <td>The zebra finch (Taeniopygia guttata) is the m...</td>\n",
              "      <td>[]</td>\n",
              "      <td>Physically assemble structure</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>346 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  ...                             single_label\n",
              "0     2324  ...                            Manage impact\n",
              "1     2362  ...                     Maintain homeostasis\n",
              "2     2367  ...         Transform radiant energy (light)\n",
              "3     2393  ...                   Modify electric charge\n",
              "4     2400  ...                    Move in/through gases\n",
              "..     ...  ...                                      ...\n",
              "341  93260  ...     Capture, absorb, or filter organisms\n",
              "342  93283  ...            Physically assemble structure\n",
              "343  93287  ...  Chemically break down organic compounds\n",
              "344  93338  ...                            Encode/Decode\n",
              "345  93416  ...            Physically assemble structure\n",
              "\n",
              "[346 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8lFgHSXKfN0",
        "outputId": "a2f7c5e4-5703-4be4-86ab-56c28a406e8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "labels = []\n",
        "docs = []\n",
        "labels_test = []\n",
        "docs_test = []\n",
        "labels_dict = ['a', 'b', 'c', 'd', 'e']\n",
        "\n",
        "single_label = labeled_df[\"single_label\"].tolist()\n",
        "wikipedia = labeled_df[\"Wikipedia\"].tolist()\n",
        "title = labeled_df[\"Title\"].tolist()\n",
        "living_systems = labeled_df[\"Living Systems\"].tolist()\n",
        "for i in range(len(title)):\n",
        "  if i < len(title) - 310:\n",
        "    docs.append(wikipedia[i])\n",
        "    #labels.append(labels_dict.index(single_label[i]))\n",
        "  else:\n",
        "    docs.append(wikipedia[i])\n",
        "    #labels_test.append(labels_dict.index(single_label[i]))\n",
        "print (len(labels))\n",
        "print (len(docs))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX5JoachgKHG"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X9M8btjgL4h"
      },
      "source": [
        "# Calculate accuracy of predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJ0gJZfGgfbt"
      },
      "source": [
        "# Format elapsed times as hh:mm:ss\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foqKWTvsNdaz"
      },
      "source": [
        "## BERT Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ21pfQvgtNA",
        "outputId": "7e8b3c58-839b-4dfe-81fb-cc3a424701c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "e33f5caa4f3a4c189159106e4c4a9ec0",
            "063cd89dee8f487c9053e6ead3aba09e",
            "318bc872f5f444188a4391a9a16cf661",
            "8bc7df069b1e492bb818904e393dc488",
            "db623ec1d1804a39b7d29e517d5302f1",
            "22d8913a90c14eccb25b7d3af6c5a102",
            "096ddbc1c5444a36afc8eb06538a25c2",
            "756cc5bbb6d44dc182ff3d2748a4815e"
          ]
        }
      },
      "source": [
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "\n",
        "# Load BERT tokenizer\n",
        "print('Loading BERT tokenizer')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e33f5caa4f3a4c189159106e4c4a9ec0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTUSIIZAhcVT",
        "outputId": "1792b9bb-e2cf-4e8f-be2a-73fbabad961c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "# Make sure it is tokenizing correctly:\n",
        "\n",
        "# Print original articles\n",
        "print(' Original: ', docs[0])\n",
        "\n",
        "# Print a doc split into tokens\n",
        "print('Tokenized: ', tokenizer.tokenize(docs[0]))\n",
        "\n",
        "# Print docs as mapped to ids\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(docs[0])))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  The toco toucan (Ramphastos toco), also known as the common toucan or giant toucan, is the largest and probably the best known species in the toucan family. It is found in semi-open habitats throughout a large part of central and eastern South America. It is a common attraction in zoos.\n",
            "\n",
            "\n",
            "== Taxonomy and systematics ==\n",
            "German zoologist Philipp Ludwig Statius Müller described the toco toucan in 1776.\n",
            "\n",
            "\n",
            "=== Subspecies ===\n",
            "Two subspecies are recognized:\n",
            "R. t. toco  - Statius Müller, 1776: Found in the Guianas, northern and north-eastern Brazil and south-eastern Peru\n",
            "R. t. albogularis - Cabanis, 1862: Originally described as a separate species. Found in eastern and southern Brazil, northern Bolivia, Paraguay and northern Argentina\n",
            "\n",
            "\n",
            "== Description ==\n",
            "The toco toucan has a striking plumage with a mainly black body, a white throat, chest and uppertail-coverts, and red undertail-coverts. What appears to be a blue iris is actually thin blue skin around the eye. This blue skin is surrounded by another ring of bare, orange skin. The most noticeable feature, however, is its huge bill, which measures from 15.8 to 23 cm (6 1⁄4 to 9 in) in length, which is yellow-orange, tending to deeper reddish-orange on its lower sections and culmen, and with a black base and large spot on the tip. It looks heavy, but as in other toucans it is relatively light because the inside largely is hollow. The tongue is nearly as long as the bill and very flat. This species is the largest toucan and the largest representative of the order Piciformes. The total length of the species is 55–65 cm (21 1⁄2–25 1⁄2 in). Body weight in these birds can vary from 500 to 876 g (1 lb 1 5⁄8 oz to 1 lb 14 7⁄8 oz), with males averaging 723 g (1 lb 9 1⁄2 oz) against the smaller female, which averages 576 g (1 lb 4 3⁄8 oz). Among standard measurements, the wing chord is 22 to 26 cm (8 1⁄2 to 10 in), the tail is 14.1 to 17.9 cm (5 9⁄16 to 7 1⁄16 in) and the tarsus is 4.8 to 6.5 cm (1 7⁄8 to 2 9⁄16 in). Other than the size difference, there are no external differences between the sexes. Juveniles are duller and shorter-billed than adults. Its voice consists of a deep, coarse croaking, often repeated every few seconds. It also has a rattling call and will bill-clack.\n",
            "The bill is the largest relative to body size of all birds providing 30 to 50% of its body surface area, although another Neotropical species, the sword-billed hummingbird, has a longer bill relative to its body length. It was called by Buffon a “grossly monstrous” appendage. Diverse functions have been suggested. Charles Darwin suggested it was a sexual ornament: “toucans may owe the enormous size of their beaks to sexual selection, for the sake of displaying the diversified and vivid stripes of colour with which these organs are ornamented\". Further suggestions have included aid in peeling fruit, intimidating other birds when robbing their nests, social selection related to defense of territory, and as a visual warning.Research has shown that one function is as a surface area for heat exchange. The bill has the ability to modify blood flow and so regulate heat distribution in the body, allowing it to use its bill as a thermal radiator. In terms of surface area used for this function, the bill relative to the bird's size is amongst the largest of any animal and has a network of superficial blood vessels supporting the thin horny sheath on the bill made of keratin called the rhamphotheca.\n",
            "In its capacity to remove body heat the bill is comparable to that of elephant ears. The ability to radiate heat depends upon air speed: if this is low only 25% of the adult bird's resting heat production to as much as four times this heat production. In comparison, the bill of a duck and the ears of elephant can shed only about 9% of resting heat production. The bill normally is responsible for 30 to 60% of heat loss. The practice of toco toucans of placing their bills under their wings may serve to insulate the bill and reduce heat loss during sleep. It has been observed that \"complexities of the vasculature and controlling mechanisms needed to adjust the blood flow to the bill may not be completely developed until\n",
            "adulthood.\"\n",
            "\n",
            "\n",
            "== Distribution and habitat ==\n",
            "The toco toucan occurs in northern and eastern Bolivia, extreme south-eastern Peru, northern Argentina, eastern and central Paraguay, and eastern and southern Brazil (excluding southern Rio Grande do Sul, the dry regions dominated by Caatinga vegetation and coastal regions between Ceará and Rio de Janeiro). Other disjunct populations occur along the lower Amazon River (Ilha de Marajó west approximately to the Madeira River), far northern Brazil in Roraima, coastal regions of the Guianas and it has been recently registered in north-west Uruguay. It only penetrates the Amazon in relatively open areas (e.g. along rivers). It is resident, but local movements may occur.\n",
            "It is, unlike the other members of the genus Ramphastos, essentially a non-forest species. It can be found in a wide range of semi-open habitats such as woodland, savanna and other open habitats with scattered trees, Cerrado, plantations, forest-edge, and even wooded gardens. It is mainly a species of lowlands, but occurs up to 1,750 m (5,740 ft) near the Andes in Bolivia. It is easily seen in the Pantanal.\n",
            "\n",
            "\n",
            "== Behaviour and ecology ==\n",
            "The toco toucan eats fruit using its bill to pluck them from trees, but also insects, frogs, small reptiles, small birds and their eggs and nestlings. The long bill is useful for reaching things that otherwise would be out-of-reach. It is typically seen in pairs or small groups. In flight it alternates between a burst of rapid flaps with the relatively short, rounded wings, and gliding. Nesting is seasonal, but timing differs between regions. The nest is typically placed high in a tree and consists of a cavity, at least part of which is excavated by the parent birds themselves. It has also been recorded nesting in holes in earth-banks and terrestrial termite-nests. Their reproduction cycle is annual. The female usually lays two to four eggs a few days after mating. The eggs are incubated by both sexes and hatch after 17–18 days. These birds are very protective of themselves and their chicks.\n",
            "\n",
            "\t\t\n",
            "\t\t\n",
            "\t\t\n",
            "\n",
            "\n",
            "== Aviculture ==\n",
            "Like the keel-billed toucan, the toco toucan is sometimes kept in captivity, but has a high fruit diet and is sensitive to hemochromatosis (an iron storage disease). Also, pet toco toucans must not be permitted to eat mouse (or rat) meat, due to a risk of bacterial infection. There is an ongoing population management plan that should help to revert the decreasing captive population of the toco toucan for Association of Zoos and Aquariums (AZA) member institutions. This is the second management plan that is occurring since 2001.\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Because it prefers open habitats, the toco toucan is likely to benefit from the widespread deforestation in tropical South America. It has a large range and except in the outer regions of its range, it typically is fairly common. It is therefore considered to be of Least Concern by BirdLife International.\n",
            "\n",
            "\n",
            "== References ==\n",
            "\n",
            "Gilbert, A. (2002). Toco Toucan (Ramphastos toco). pp. 270–271 in: del Hoyo, J., Elliott, A. & Sargatal, J. eds (2002). Handbook of Birds of the World. Vol. 7. Jacamars to Woodpeckers. Lynx Edicions, Barcelona. ISBN 84-87334-37-7\n",
            "Restall, R., Rodner, C. & Lentino, M. (2006). Birds of Northern South America - An Identification Guide. Christopher Helm, London. ISBN 0-7136-7242-0\n",
            "Short, L. & Horne, J. (2001). Toucans, Barbets and Honeyguides. Oxford University Press, London. ISBN 0-19-854666-1\n",
            "Sick, H. (1993). Birds of Brazil - A Natural History. Princeton University Press, West Sussex. ISBN 0-691-08569-2\n",
            "\n",
            "\n",
            "== External links ==\n",
            "Toco toucan videos on the Internet Bird Collection\n",
            "Stamps (for Argentina, Bolivia, Brazil, French Guiana, Guyana, Paraguay)\n",
            "Tokenized:  ['the', 'to', '##co', 'to', '##uca', '##n', '(', 'ramp', '##has', '##tos', 'to', '##co', ')', ',', 'also', 'known', 'as', 'the', 'common', 'to', '##uca', '##n', 'or', 'giant', 'to', '##uca', '##n', ',', 'is', 'the', 'largest', 'and', 'probably', 'the', 'best', 'known', 'species', 'in', 'the', 'to', '##uca', '##n', 'family', '.', 'it', 'is', 'found', 'in', 'semi', '-', 'open', 'habitats', 'throughout', 'a', 'large', 'part', 'of', 'central', 'and', 'eastern', 'south', 'america', '.', 'it', 'is', 'a', 'common', 'attraction', 'in', 'zoo', '##s', '.', '=', '=', 'taxonomy', 'and', 'systematic', '##s', '=', '=', 'german', 'zoo', '##logist', 'philipp', 'ludwig', 'stat', '##ius', 'muller', 'described', 'the', 'to', '##co', 'to', '##uca', '##n', 'in', '1776', '.', '=', '=', '=', 'subspecies', '=', '=', '=', 'two', 'subspecies', 'are', 'recognized', ':', 'r', '.', 't', '.', 'to', '##co', '-', 'stat', '##ius', 'muller', ',', '1776', ':', 'found', 'in', 'the', 'guiana', '##s', ',', 'northern', 'and', 'north', '-', 'eastern', 'brazil', 'and', 'south', '-', 'eastern', 'peru', 'r', '.', 't', '.', 'al', '##bo', '##gul', '##aris', '-', 'cab', '##ani', '##s', ',', '1862', ':', 'originally', 'described', 'as', 'a', 'separate', 'species', '.', 'found', 'in', 'eastern', 'and', 'southern', 'brazil', ',', 'northern', 'bolivia', ',', 'paraguay', 'and', 'northern', 'argentina', '=', '=', 'description', '=', '=', 'the', 'to', '##co', 'to', '##uca', '##n', 'has', 'a', 'striking', 'plumage', 'with', 'a', 'mainly', 'black', 'body', ',', 'a', 'white', 'throat', ',', 'chest', 'and', 'upper', '##tail', '-', 'covert', '##s', ',', 'and', 'red', 'under', '##tail', '-', 'covert', '##s', '.', 'what', 'appears', 'to', 'be', 'a', 'blue', 'iris', 'is', 'actually', 'thin', 'blue', 'skin', 'around', 'the', 'eye', '.', 'this', 'blue', 'skin', 'is', 'surrounded', 'by', 'another', 'ring', 'of', 'bare', ',', 'orange', 'skin', '.', 'the', 'most', 'noticeable', 'feature', ',', 'however', ',', 'is', 'its', 'huge', 'bill', ',', 'which', 'measures', 'from', '15', '.', '8', 'to', '23', 'cm', '(', '6', '1', '##⁄', '##4', 'to', '9', 'in', ')', 'in', 'length', ',', 'which', 'is', 'yellow', '-', 'orange', ',', 'tending', 'to', 'deeper', 'reddish', '-', 'orange', 'on', 'its', 'lower', 'sections', 'and', 'cu', '##lm', '##en', ',', 'and', 'with', 'a', 'black', 'base', 'and', 'large', 'spot', 'on', 'the', 'tip', '.', 'it', 'looks', 'heavy', ',', 'but', 'as', 'in', 'other', 'to', '##uca', '##ns', 'it', 'is', 'relatively', 'light', 'because', 'the', 'inside', 'largely', 'is', 'hollow', '.', 'the', 'tongue', 'is', 'nearly', 'as', 'long', 'as', 'the', 'bill', 'and', 'very', 'flat', '.', 'this', 'species', 'is', 'the', 'largest', 'to', '##uca', '##n', 'and', 'the', 'largest', 'representative', 'of', 'the', 'order', 'pic', '##iform', '##es', '.', 'the', 'total', 'length', 'of', 'the', 'species', 'is', '55', '–', '65', 'cm', '(', '21', '1', '##⁄', '##2', '–', '25', '1', '##⁄', '##2', 'in', ')', '.', 'body', 'weight', 'in', 'these', 'birds', 'can', 'vary', 'from', '500', 'to', '87', '##6', 'g', '(', '1', 'lb', '1', '5', '##⁄', '##8', 'oz', 'to', '1', 'lb', '14', '7', '##⁄', '##8', 'oz', ')', ',', 'with', 'males', 'averaging', '72', '##3', 'g', '(', '1', 'lb', '9', '1', '##⁄', '##2', 'oz', ')', 'against', 'the', 'smaller', 'female', ',', 'which', 'averages', '57', '##6', 'g', '(', '1', 'lb', '4', '3', '##⁄', '##8', 'oz', ')', '.', 'among', 'standard', 'measurements', ',', 'the', 'wing', 'chord', 'is', '22', 'to', '26', 'cm', '(', '8', '1', '##⁄', '##2', 'to', '10', 'in', ')', ',', 'the', 'tail', 'is', '14', '.', '1', 'to', '17', '.', '9', 'cm', '(', '5', '9', '##⁄', '##16', 'to', '7', '1', '##⁄', '##16', 'in', ')', 'and', 'the', 'tar', '##sus', 'is', '4', '.', '8', 'to', '6', '.', '5', 'cm', '(', '1', '7', '##⁄', '##8', 'to', '2', '9', '##⁄', '##16', 'in', ')', '.', 'other', 'than', 'the', 'size', 'difference', ',', 'there', 'are', 'no', 'external', 'differences', 'between', 'the', 'sexes', '.', 'juveniles', 'are', 'dull', '##er', 'and', 'shorter', '-', 'billed', 'than', 'adults', '.', 'its', 'voice', 'consists', 'of', 'a', 'deep', ',', 'coarse', 'cr', '##oa', '##king', ',', 'often', 'repeated', 'every', 'few', 'seconds', '.', 'it', 'also', 'has', 'a', 'rattling', 'call', 'and', 'will', 'bill', '-', 'cl', '##ack', '.', 'the', 'bill', 'is', 'the', 'largest', 'relative', 'to', 'body', 'size', 'of', 'all', 'birds', 'providing', '30', 'to', '50', '%', 'of', 'its', 'body', 'surface', 'area', ',', 'although', 'another', 'neo', '##tropical', 'species', ',', 'the', 'sword', '-', 'billed', 'humming', '##bird', ',', 'has', 'a', 'longer', 'bill', 'relative', 'to', 'its', 'body', 'length', '.', 'it', 'was', 'called', 'by', 'buff', '##on', 'a', '“', 'gross', '##ly', 'monstrous', '”', 'app', '##end', '##age', '.', 'diverse', 'functions', 'have', 'been', 'suggested', '.', 'charles', 'darwin', 'suggested', 'it', 'was', 'a', 'sexual', 'or', '##name', '##nt', ':', '“', 'to', '##uca', '##ns', 'may', 'owe', 'the', 'enormous', 'size', 'of', 'their', 'beak', '##s', 'to', 'sexual', 'selection', ',', 'for', 'the', 'sake', 'of', 'displaying', 'the', 'diversified', 'and', 'vivid', 'stripes', 'of', 'colour', 'with', 'which', 'these', 'organs', 'are', 'or', '##name', '##nted', '\"', '.', 'further', 'suggestions', 'have', 'included', 'aid', 'in', 'peeling', 'fruit', ',', 'intimidating', 'other', 'birds', 'when', 'robb', '##ing', 'their', 'nests', ',', 'social', 'selection', 'related', 'to', 'defense', 'of', 'territory', ',', 'and', 'as', 'a', 'visual', 'warning', '.', 'research', 'has', 'shown', 'that', 'one', 'function', 'is', 'as', 'a', 'surface', 'area', 'for', 'heat', 'exchange', '.', 'the', 'bill', 'has', 'the', 'ability', 'to', 'modify', 'blood', 'flow', 'and', 'so', 'regulate', 'heat', 'distribution', 'in', 'the', 'body', ',', 'allowing', 'it', 'to', 'use', 'its', 'bill', 'as', 'a', 'thermal', 'ra', '##dia', '##tor', '.', 'in', 'terms', 'of', 'surface', 'area', 'used', 'for', 'this', 'function', ',', 'the', 'bill', 'relative', 'to', 'the', 'bird', \"'\", 's', 'size', 'is', 'amongst', 'the', 'largest', 'of', 'any', 'animal', 'and', 'has', 'a', 'network', 'of', 'superficial', 'blood', 'vessels', 'supporting', 'the', 'thin', 'horn', '##y', 'sheath', 'on', 'the', 'bill', 'made', 'of', 'ke', '##rat', '##in', 'called', 'the', 'r', '##ham', '##ph', '##oth', '##eca', '.', 'in', 'its', 'capacity', 'to', 'remove', 'body', 'heat', 'the', 'bill', 'is', 'comparable', 'to', 'that', 'of', 'elephant', 'ears', '.', 'the', 'ability', 'to', 'ra', '##dia', '##te', 'heat', 'depends', 'upon', 'air', 'speed', ':', 'if', 'this', 'is', 'low', 'only', '25', '%', 'of', 'the', 'adult', 'bird', \"'\", 's', 'resting', 'heat', 'production', 'to', 'as', 'much', 'as', 'four', 'times', 'this', 'heat', 'production', '.', 'in', 'comparison', ',', 'the', 'bill', 'of', 'a', 'duck', 'and', 'the', 'ears', 'of', 'elephant', 'can', 'shed', 'only', 'about', '9', '%', 'of', 'resting', 'heat', 'production', '.', 'the', 'bill', 'normally', 'is', 'responsible', 'for', '30', 'to', '60', '%', 'of', 'heat', 'loss', '.', 'the', 'practice', 'of', 'to', '##co', 'to', '##uca', '##ns', 'of', 'placing', 'their', 'bills', 'under', 'their', 'wings', 'may', 'serve', 'to', 'ins', '##ulate', 'the', 'bill', 'and', 'reduce', 'heat', 'loss', 'during', 'sleep', '.', 'it', 'has', 'been', 'observed', 'that', '\"', 'complex', '##ities', 'of', 'the', 'va', '##scu', '##lat', '##ure', 'and', 'controlling', 'mechanisms', 'needed', 'to', 'adjust', 'the', 'blood', 'flow', 'to', 'the', 'bill', 'may', 'not', 'be', 'completely', 'developed', 'until', 'adulthood', '.', '\"', '=', '=', 'distribution', 'and', 'habitat', '=', '=', 'the', 'to', '##co', 'to', '##uca', '##n', 'occurs', 'in', 'northern', 'and', 'eastern', 'bolivia', ',', 'extreme', 'south', '-', 'eastern', 'peru', ',', 'northern', 'argentina', ',', 'eastern', 'and', 'central', 'paraguay', ',', 'and', 'eastern', 'and', 'southern', 'brazil', '(', 'excluding', 'southern', 'rio', 'grande', 'do', 'sul', ',', 'the', 'dry', 'regions', 'dominated', 'by', 'ca', '##ating', '##a', 'vegetation', 'and', 'coastal', 'regions', 'between', 'ce', '##ara', 'and', 'rio', 'de', 'janeiro', ')', '.', 'other', 'di', '##s', '##jun', '##ct', 'populations', 'occur', 'along', 'the', 'lower', 'amazon', 'river', '(', 'il', '##ha', 'de', 'mara', '##jo', 'west', 'approximately', 'to', 'the', 'madeira', 'river', ')', ',', 'far', 'northern', 'brazil', 'in', 'ro', '##rai', '##ma', ',', 'coastal', 'regions', 'of', 'the', 'guiana', '##s', 'and', 'it', 'has', 'been', 'recently', 'registered', 'in', 'north', '-', 'west', 'uruguay', '.', 'it', 'only', 'penetrate', '##s', 'the', 'amazon', 'in', 'relatively', 'open', 'areas', '(', 'e', '.', 'g', '.', 'along', 'rivers', ')', '.', 'it', 'is', 'resident', ',', 'but', 'local', 'movements', 'may', 'occur', '.', 'it', 'is', ',', 'unlike', 'the', 'other', 'members', 'of', 'the', 'genus', 'ramp', '##has', '##tos', ',', 'essentially', 'a', 'non', '-', 'forest', 'species', '.', 'it', 'can', 'be', 'found', 'in', 'a', 'wide', 'range', 'of', 'semi', '-', 'open', 'habitats', 'such', 'as', 'woodland', ',', 'savanna', 'and', 'other', 'open', 'habitats', 'with', 'scattered', 'trees', ',', 'ce', '##rrado', ',', 'plantations', ',', 'forest', '-', 'edge', ',', 'and', 'even', 'wooded', 'gardens', '.', 'it', 'is', 'mainly', 'a', 'species', 'of', 'lowlands', ',', 'but', 'occurs', 'up', 'to', '1', ',', '750', 'm', '(', '5', ',', '740', 'ft', ')', 'near', 'the', 'andes', 'in', 'bolivia', '.', 'it', 'is', 'easily', 'seen', 'in', 'the', 'pan', '##tana', '##l', '.', '=', '=', 'behaviour', 'and', 'ecology', '=', '=', 'the', 'to', '##co', 'to', '##uca', '##n', 'eats', 'fruit', 'using', 'its', 'bill', 'to', 'pl', '##uck', 'them', 'from', 'trees', ',', 'but', 'also', 'insects', ',', 'frogs', ',', 'small', 'reptiles', ',', 'small', 'birds', 'and', 'their', 'eggs', 'and', 'nest', '##lings', '.', 'the', 'long', 'bill', 'is', 'useful', 'for', 'reaching', 'things', 'that', 'otherwise', 'would', 'be', 'out', '-', 'of', '-', 'reach', '.', 'it', 'is', 'typically', 'seen', 'in', 'pairs', 'or', 'small', 'groups', '.', 'in', 'flight', 'it', 'alternate', '##s', 'between', 'a', 'burst', 'of', 'rapid', 'flaps', 'with', 'the', 'relatively', 'short', ',', 'rounded', 'wings', ',', 'and', 'gliding', '.', 'nesting', 'is', 'seasonal', ',', 'but', 'timing', 'differs', 'between', 'regions', '.', 'the', 'nest', 'is', 'typically', 'placed', 'high', 'in', 'a', 'tree', 'and', 'consists', 'of', 'a', 'cavity', ',', 'at', 'least', 'part', 'of', 'which', 'is', 'excavated', 'by', 'the', 'parent', 'birds', 'themselves', '.', 'it', 'has', 'also', 'been', 'recorded', 'nesting', 'in', 'holes', 'in', 'earth', '-', 'banks', 'and', 'terrestrial', 'term', '##ite', '-', 'nests', '.', 'their', 'reproduction', 'cycle', 'is', 'annual', '.', 'the', 'female', 'usually', 'lays', 'two', 'to', 'four', 'eggs', 'a', 'few', 'days', 'after', 'mating', '.', 'the', 'eggs', 'are', 'inc', '##uba', '##ted', 'by', 'both', 'sexes', 'and', 'hatch', 'after', '17', '–', '18', 'days', '.', 'these', 'birds', 'are', 'very', 'protective', 'of', 'themselves', 'and', 'their', 'chicks', '.', '=', '=', 'av', '##ic', '##ult', '##ure', '=', '=', 'like', 'the', 'keel', '-', 'billed', 'to', '##uca', '##n', ',', 'the', 'to', '##co', 'to', '##uca', '##n', 'is', 'sometimes', 'kept', 'in', 'captivity', ',', 'but', 'has', 'a', 'high', 'fruit', 'diet', 'and', 'is', 'sensitive', 'to', 'hem', '##och', '##rom', '##ato', '##sis', '(', 'an', 'iron', 'storage', 'disease', ')', '.', 'also', ',', 'pet', 'to', '##co', 'to', '##uca', '##ns', 'must', 'not', 'be', 'permitted', 'to', 'eat', 'mouse', '(', 'or', 'rat', ')', 'meat', ',', 'due', 'to', 'a', 'risk', 'of', 'bacterial', 'infection', '.', 'there', 'is', 'an', 'ongoing', 'population', 'management', 'plan', 'that', 'should', 'help', 'to', 'rev', '##ert', 'the', 'decreasing', 'captive', 'population', 'of', 'the', 'to', '##co', 'to', '##uca', '##n', 'for', 'association', 'of', 'zoo', '##s', 'and', 'aquarium', '##s', '(', 'az', '##a', ')', 'member', 'institutions', '.', 'this', 'is', 'the', 'second', 'management', 'plan', 'that', 'is', 'occurring', 'since', '2001', '.', '=', '=', 'status', '=', '=', 'because', 'it', 'prefers', 'open', 'habitats', ',', 'the', 'to', '##co', 'to', '##uca', '##n', 'is', 'likely', 'to', 'benefit', 'from', 'the', 'widespread', 'def', '##orestation', 'in', 'tropical', 'south', 'america', '.', 'it', 'has', 'a', 'large', 'range', 'and', 'except', 'in', 'the', 'outer', 'regions', 'of', 'its', 'range', ',', 'it', 'typically', 'is', 'fairly', 'common', '.', 'it', 'is', 'therefore', 'considered', 'to', 'be', 'of', 'least', 'concern', 'by', 'bird', '##life', 'international', '.', '=', '=', 'references', '=', '=', 'gilbert', ',', 'a', '.', '(', '2002', ')', '.', 'to', '##co', 'to', '##uca', '##n', '(', 'ramp', '##has', '##tos', 'to', '##co', ')', '.', 'pp', '.', '270', '–', '271', 'in', ':', 'del', 'ho', '##yo', ',', 'j', '.', ',', 'elliott', ',', 'a', '.', '&', 'sar', '##gata', '##l', ',', 'j', '.', 'eds', '(', '2002', ')', '.', 'handbook', 'of', 'birds', 'of', 'the', 'world', '.', 'vol', '.', '7', '.', 'ja', '##cam', '##ars', 'to', 'wood', '##pe', '##cker', '##s', '.', 'lynx', 'ed', '##icio', '##ns', ',', 'barcelona', '.', 'isbn', '84', '-', '87', '##33', '##4', '-', '37', '-', '7', 'rest', '##all', ',', 'r', '.', ',', 'rod', '##ner', ',', 'c', '.', '&', 'lent', '##ino', ',', 'm', '.', '(', '2006', ')', '.', 'birds', 'of', 'northern', 'south', 'america', '-', 'an', 'identification', 'guide', '.', 'christopher', 'helm', ',', 'london', '.', 'isbn', '0', '-', '71', '##36', '-', '72', '##42', '-', '0', 'short', ',', 'l', '.', '&', 'horne', ',', 'j', '.', '(', '2001', ')', '.', 'to', '##uca', '##ns', ',', 'bar', '##bet', '##s', 'and', 'honey', '##guide', '##s', '.', 'oxford', 'university', 'press', ',', 'london', '.', 'isbn', '0', '-', '19', '-', '85', '##46', '##66', '-', '1', 'sick', ',', 'h', '.', '(', '1993', ')', '.', 'birds', 'of', 'brazil', '-', 'a', 'natural', 'history', '.', 'princeton', 'university', 'press', ',', 'west', 'sussex', '.', 'isbn', '0', '-', '69', '##1', '-', '08', '##56', '##9', '-', '2', '=', '=', 'external', 'links', '=', '=', 'to', '##co', 'to', '##uca', '##n', 'videos', 'on', 'the', 'internet', 'bird', 'collection', 'stamps', '(', 'for', 'argentina', ',', 'bolivia', ',', 'brazil', ',', 'french', 'guiana', ',', 'guyana', ',', 'paraguay', ')']\n",
            "Token IDs:  [1996, 2000, 3597, 2000, 18100, 2078, 1006, 13276, 14949, 13122, 2000, 3597, 1007, 1010, 2036, 2124, 2004, 1996, 2691, 2000, 18100, 2078, 2030, 5016, 2000, 18100, 2078, 1010, 2003, 1996, 2922, 1998, 2763, 1996, 2190, 2124, 2427, 1999, 1996, 2000, 18100, 2078, 2155, 1012, 2009, 2003, 2179, 1999, 4100, 1011, 2330, 10746, 2802, 1037, 2312, 2112, 1997, 2430, 1998, 2789, 2148, 2637, 1012, 2009, 2003, 1037, 2691, 8432, 1999, 9201, 2015, 1012, 1027, 1027, 25274, 1998, 11778, 2015, 1027, 1027, 2446, 9201, 10727, 20765, 10302, 28093, 4173, 12304, 2649, 1996, 2000, 3597, 2000, 18100, 2078, 1999, 13963, 1012, 1027, 1027, 1027, 11056, 1027, 1027, 1027, 2048, 11056, 2024, 3858, 1024, 1054, 1012, 1056, 1012, 2000, 3597, 1011, 28093, 4173, 12304, 1010, 13963, 1024, 2179, 1999, 1996, 23568, 2015, 1010, 2642, 1998, 2167, 1011, 2789, 4380, 1998, 2148, 1011, 2789, 7304, 1054, 1012, 1056, 1012, 2632, 5092, 24848, 23061, 1011, 9298, 7088, 2015, 1010, 6889, 1024, 2761, 2649, 2004, 1037, 3584, 2427, 1012, 2179, 1999, 2789, 1998, 2670, 4380, 1010, 2642, 11645, 1010, 13884, 1998, 2642, 5619, 1027, 1027, 6412, 1027, 1027, 1996, 2000, 3597, 2000, 18100, 2078, 2038, 1037, 8478, 28764, 2007, 1037, 3701, 2304, 2303, 1010, 1037, 2317, 3759, 1010, 3108, 1998, 3356, 14162, 1011, 19813, 2015, 1010, 1998, 2417, 2104, 14162, 1011, 19813, 2015, 1012, 2054, 3544, 2000, 2022, 1037, 2630, 11173, 2003, 2941, 4857, 2630, 3096, 2105, 1996, 3239, 1012, 2023, 2630, 3096, 2003, 5129, 2011, 2178, 3614, 1997, 6436, 1010, 4589, 3096, 1012, 1996, 2087, 17725, 3444, 1010, 2174, 1010, 2003, 2049, 4121, 3021, 1010, 2029, 5761, 2013, 2321, 1012, 1022, 2000, 2603, 4642, 1006, 1020, 1015, 30070, 2549, 2000, 1023, 1999, 1007, 1999, 3091, 1010, 2029, 2003, 3756, 1011, 4589, 1010, 25069, 2000, 6748, 14182, 1011, 4589, 2006, 2049, 2896, 5433, 1998, 12731, 13728, 2368, 1010, 1998, 2007, 1037, 2304, 2918, 1998, 2312, 3962, 2006, 1996, 5955, 1012, 2009, 3504, 3082, 1010, 2021, 2004, 1999, 2060, 2000, 18100, 3619, 2009, 2003, 4659, 2422, 2138, 1996, 2503, 4321, 2003, 8892, 1012, 1996, 4416, 2003, 3053, 2004, 2146, 2004, 1996, 3021, 1998, 2200, 4257, 1012, 2023, 2427, 2003, 1996, 2922, 2000, 18100, 2078, 1998, 1996, 2922, 4387, 1997, 1996, 2344, 27263, 22631, 2229, 1012, 1996, 2561, 3091, 1997, 1996, 2427, 2003, 4583, 1516, 3515, 4642, 1006, 2538, 1015, 30070, 2475, 1516, 2423, 1015, 30070, 2475, 1999, 1007, 1012, 2303, 3635, 1999, 2122, 5055, 2064, 8137, 2013, 3156, 2000, 6584, 2575, 1043, 1006, 1015, 6053, 1015, 1019, 30070, 2620, 11472, 2000, 1015, 6053, 2403, 1021, 30070, 2620, 11472, 1007, 1010, 2007, 3767, 14985, 5824, 2509, 1043, 1006, 1015, 6053, 1023, 1015, 30070, 2475, 11472, 1007, 2114, 1996, 3760, 2931, 1010, 2029, 20185, 5401, 2575, 1043, 1006, 1015, 6053, 1018, 1017, 30070, 2620, 11472, 1007, 1012, 2426, 3115, 11702, 1010, 1996, 3358, 13924, 2003, 2570, 2000, 2656, 4642, 1006, 1022, 1015, 30070, 2475, 2000, 2184, 1999, 1007, 1010, 1996, 5725, 2003, 2403, 1012, 1015, 2000, 2459, 1012, 1023, 4642, 1006, 1019, 1023, 30070, 16048, 2000, 1021, 1015, 30070, 16048, 1999, 1007, 1998, 1996, 16985, 13203, 2003, 1018, 1012, 1022, 2000, 1020, 1012, 1019, 4642, 1006, 1015, 1021, 30070, 2620, 2000, 1016, 1023, 30070, 16048, 1999, 1007, 1012, 2060, 2084, 1996, 2946, 4489, 1010, 2045, 2024, 2053, 6327, 5966, 2090, 1996, 21024, 1012, 25406, 2024, 10634, 2121, 1998, 7820, 1011, 14843, 2084, 6001, 1012, 2049, 2376, 3774, 1997, 1037, 2784, 1010, 20392, 13675, 10441, 6834, 1010, 2411, 5567, 2296, 2261, 3823, 1012, 2009, 2036, 2038, 1037, 26347, 2655, 1998, 2097, 3021, 1011, 18856, 8684, 1012, 1996, 3021, 2003, 1996, 2922, 5816, 2000, 2303, 2946, 1997, 2035, 5055, 4346, 2382, 2000, 2753, 1003, 1997, 2049, 2303, 3302, 2181, 1010, 2348, 2178, 9253, 25528, 2427, 1010, 1996, 4690, 1011, 14843, 20364, 9001, 1010, 2038, 1037, 2936, 3021, 5816, 2000, 2049, 2303, 3091, 1012, 2009, 2001, 2170, 2011, 23176, 2239, 1037, 1523, 7977, 2135, 21668, 1524, 10439, 10497, 4270, 1012, 7578, 4972, 2031, 2042, 4081, 1012, 2798, 11534, 4081, 2009, 2001, 1037, 4424, 2030, 18442, 3372, 1024, 1523, 2000, 18100, 3619, 2089, 12533, 1996, 8216, 2946, 1997, 2037, 23525, 2015, 2000, 4424, 4989, 1010, 2005, 1996, 8739, 1997, 14962, 1996, 24908, 1998, 14954, 13560, 1997, 6120, 2007, 2029, 2122, 11595, 2024, 2030, 18442, 14706, 1000, 1012, 2582, 15690, 2031, 2443, 4681, 1999, 28241, 5909, 1010, 24439, 2060, 5055, 2043, 26211, 2075, 2037, 17415, 1010, 2591, 4989, 3141, 2000, 3639, 1997, 3700, 1010, 1998, 2004, 1037, 5107, 5432, 1012, 2470, 2038, 3491, 2008, 2028, 3853, 2003, 2004, 1037, 3302, 2181, 2005, 3684, 3863, 1012, 1996, 3021, 2038, 1996, 3754, 2000, 19933, 2668, 4834, 1998, 2061, 15176, 3684, 4353, 1999, 1996, 2303, 1010, 4352, 2009, 2000, 2224, 2049, 3021, 2004, 1037, 9829, 10958, 9032, 4263, 1012, 1999, 3408, 1997, 3302, 2181, 2109, 2005, 2023, 3853, 1010, 1996, 3021, 5816, 2000, 1996, 4743, 1005, 1055, 2946, 2003, 5921, 1996, 2922, 1997, 2151, 4111, 1998, 2038, 1037, 2897, 1997, 23105, 2668, 6470, 4637, 1996, 4857, 7109, 2100, 21867, 2006, 1996, 3021, 2081, 1997, 17710, 8609, 2378, 2170, 1996, 1054, 3511, 8458, 14573, 19281, 1012, 1999, 2049, 3977, 2000, 6366, 2303, 3684, 1996, 3021, 2003, 12435, 2000, 2008, 1997, 10777, 5551, 1012, 1996, 3754, 2000, 10958, 9032, 2618, 3684, 9041, 2588, 2250, 3177, 1024, 2065, 2023, 2003, 2659, 2069, 2423, 1003, 1997, 1996, 4639, 4743, 1005, 1055, 8345, 3684, 2537, 2000, 2004, 2172, 2004, 2176, 2335, 2023, 3684, 2537, 1012, 1999, 7831, 1010, 1996, 3021, 1997, 1037, 9457, 1998, 1996, 5551, 1997, 10777, 2064, 8328, 2069, 2055, 1023, 1003, 1997, 8345, 3684, 2537, 1012, 1996, 3021, 5373, 2003, 3625, 2005, 2382, 2000, 3438, 1003, 1997, 3684, 3279, 1012, 1996, 3218, 1997, 2000, 3597, 2000, 18100, 3619, 1997, 6885, 2037, 8236, 2104, 2037, 4777, 2089, 3710, 2000, 16021, 9869, 1996, 3021, 1998, 5547, 3684, 3279, 2076, 3637, 1012, 2009, 2038, 2042, 5159, 2008, 1000, 3375, 6447, 1997, 1996, 12436, 28817, 20051, 5397, 1998, 9756, 10595, 2734, 2000, 14171, 1996, 2668, 4834, 2000, 1996, 3021, 2089, 2025, 2022, 3294, 2764, 2127, 20480, 1012, 1000, 1027, 1027, 4353, 1998, 6552, 1027, 1027, 1996, 2000, 3597, 2000, 18100, 2078, 5158, 1999, 2642, 1998, 2789, 11645, 1010, 6034, 2148, 1011, 2789, 7304, 1010, 2642, 5619, 1010, 2789, 1998, 2430, 13884, 1010, 1998, 2789, 1998, 2670, 4380, 1006, 13343, 2670, 5673, 9026, 2079, 21396, 1010, 1996, 4318, 4655, 6817, 2011, 6187, 5844, 2050, 10072, 1998, 5780, 4655, 2090, 8292, 5400, 1998, 5673, 2139, 11497, 1007, 1012, 2060, 4487, 2015, 19792, 6593, 7080, 5258, 2247, 1996, 2896, 9733, 2314, 1006, 6335, 3270, 2139, 13955, 5558, 2225, 3155, 2000, 1996, 27309, 2314, 1007, 1010, 2521, 2642, 4380, 1999, 20996, 14995, 2863, 1010, 5780, 4655, 1997, 1996, 23568, 2015, 1998, 2009, 2038, 2042, 3728, 5068, 1999, 2167, 1011, 2225, 11724, 1012, 2009, 2069, 19136, 2015, 1996, 9733, 1999, 4659, 2330, 2752, 1006, 1041, 1012, 1043, 1012, 2247, 5485, 1007, 1012, 2009, 2003, 6319, 1010, 2021, 2334, 5750, 2089, 5258, 1012, 2009, 2003, 1010, 4406, 1996, 2060, 2372, 1997, 1996, 3562, 13276, 14949, 13122, 1010, 7687, 1037, 2512, 1011, 3224, 2427, 1012, 2009, 2064, 2022, 2179, 1999, 1037, 2898, 2846, 1997, 4100, 1011, 2330, 10746, 2107, 2004, 11051, 1010, 22323, 1998, 2060, 2330, 10746, 2007, 7932, 3628, 1010, 8292, 27933, 1010, 14979, 1010, 3224, 1011, 3341, 1010, 1998, 2130, 17172, 5822, 1012, 2009, 2003, 3701, 1037, 2427, 1997, 26411, 1010, 2021, 5158, 2039, 2000, 1015, 1010, 9683, 1049, 1006, 1019, 1010, 25833, 3027, 1007, 2379, 1996, 18096, 1999, 11645, 1012, 2009, 2003, 4089, 2464, 1999, 1996, 6090, 20496, 2140, 1012, 1027, 1027, 9164, 1998, 13517, 1027, 1027, 1996, 2000, 3597, 2000, 18100, 2078, 20323, 5909, 2478, 2049, 3021, 2000, 20228, 12722, 2068, 2013, 3628, 1010, 2021, 2036, 9728, 1010, 17582, 1010, 2235, 20978, 1010, 2235, 5055, 1998, 2037, 6763, 1998, 9089, 11227, 1012, 1996, 2146, 3021, 2003, 6179, 2005, 4285, 2477, 2008, 4728, 2052, 2022, 2041, 1011, 1997, 1011, 3362, 1012, 2009, 2003, 4050, 2464, 1999, 7689, 2030, 2235, 2967, 1012, 1999, 3462, 2009, 6585, 2015, 2090, 1037, 6532, 1997, 5915, 26570, 2007, 1996, 4659, 2460, 1010, 8352, 4777, 1010, 1998, 20292, 1012, 21016, 2003, 12348, 1010, 2021, 10984, 12980, 2090, 4655, 1012, 1996, 9089, 2003, 4050, 2872, 2152, 1999, 1037, 3392, 1998, 3774, 1997, 1037, 17790, 1010, 2012, 2560, 2112, 1997, 2029, 2003, 15199, 2011, 1996, 6687, 5055, 3209, 1012, 2009, 2038, 2036, 2042, 2680, 21016, 1999, 8198, 1999, 3011, 1011, 5085, 1998, 12350, 2744, 4221, 1011, 17415, 1012, 2037, 14627, 5402, 2003, 3296, 1012, 1996, 2931, 2788, 19764, 2048, 2000, 2176, 6763, 1037, 2261, 2420, 2044, 15100, 1012, 1996, 6763, 2024, 4297, 19761, 3064, 2011, 2119, 21024, 1998, 11300, 2044, 2459, 1516, 2324, 2420, 1012, 2122, 5055, 2024, 2200, 9474, 1997, 3209, 1998, 2037, 20649, 1012, 1027, 1027, 20704, 2594, 11314, 5397, 1027, 1027, 2066, 1996, 19602, 1011, 14843, 2000, 18100, 2078, 1010, 1996, 2000, 3597, 2000, 18100, 2078, 2003, 2823, 2921, 1999, 16187, 1010, 2021, 2038, 1037, 2152, 5909, 8738, 1998, 2003, 7591, 2000, 19610, 11663, 21716, 10610, 6190, 1006, 2019, 3707, 5527, 4295, 1007, 1012, 2036, 1010, 9004, 2000, 3597, 2000, 18100, 3619, 2442, 2025, 2022, 7936, 2000, 4521, 8000, 1006, 2030, 9350, 1007, 6240, 1010, 2349, 2000, 1037, 3891, 1997, 17341, 8985, 1012, 2045, 2003, 2019, 7552, 2313, 2968, 2933, 2008, 2323, 2393, 2000, 7065, 8743, 1996, 16922, 12481, 2313, 1997, 1996, 2000, 3597, 2000, 18100, 2078, 2005, 2523, 1997, 9201, 2015, 1998, 18257, 2015, 1006, 17207, 2050, 1007, 2266, 4896, 1012, 2023, 2003, 1996, 2117, 2968, 2933, 2008, 2003, 10066, 2144, 2541, 1012, 1027, 1027, 3570, 1027, 1027, 2138, 2009, 19233, 2330, 10746, 1010, 1996, 2000, 3597, 2000, 18100, 2078, 2003, 3497, 2000, 5770, 2013, 1996, 6923, 13366, 25794, 1999, 5133, 2148, 2637, 1012, 2009, 2038, 1037, 2312, 2846, 1998, 3272, 1999, 1996, 6058, 4655, 1997, 2049, 2846, 1010, 2009, 4050, 2003, 7199, 2691, 1012, 2009, 2003, 3568, 2641, 2000, 2022, 1997, 2560, 5142, 2011, 4743, 15509, 2248, 1012, 1027, 1027, 7604, 1027, 1027, 7664, 1010, 1037, 1012, 1006, 2526, 1007, 1012, 2000, 3597, 2000, 18100, 2078, 1006, 13276, 14949, 13122, 2000, 3597, 1007, 1012, 4903, 1012, 13756, 1516, 25103, 1999, 1024, 3972, 7570, 7677, 1010, 1046, 1012, 1010, 9899, 1010, 1037, 1012, 1004, 18906, 26589, 2140, 1010, 1046, 1012, 11985, 1006, 2526, 1007, 1012, 14812, 1997, 5055, 1997, 1996, 2088, 1012, 5285, 1012, 1021, 1012, 14855, 28727, 11650, 2000, 3536, 5051, 9102, 2015, 1012, 22636, 3968, 27113, 3619, 1010, 7623, 1012, 3175, 6391, 1011, 6584, 22394, 2549, 1011, 4261, 1011, 1021, 2717, 8095, 1010, 1054, 1012, 1010, 8473, 3678, 1010, 1039, 1012, 1004, 15307, 5740, 1010, 1049, 1012, 1006, 2294, 1007, 1012, 5055, 1997, 2642, 2148, 2637, 1011, 2019, 8720, 5009, 1012, 5696, 16254, 1010, 2414, 1012, 3175, 1014, 1011, 6390, 21619, 1011, 5824, 20958, 1011, 1014, 2460, 1010, 1048, 1012, 1004, 24084, 1010, 1046, 1012, 1006, 2541, 1007, 1012, 2000, 18100, 3619, 1010, 3347, 20915, 2015, 1998, 6861, 28582, 2015, 1012, 4345, 2118, 2811, 1010, 2414, 1012, 3175, 1014, 1011, 2539, 1011, 5594, 21472, 28756, 1011, 1015, 5305, 1010, 1044, 1012, 1006, 2857, 1007, 1012, 5055, 1997, 4380, 1011, 1037, 3019, 2381, 1012, 9173, 2118, 2811, 1010, 2225, 9503, 1012, 3175, 1014, 1011, 6353, 2487, 1011, 5511, 26976, 2683, 1011, 1016, 1027, 1027, 6327, 6971, 1027, 1027, 2000, 3597, 2000, 18100, 2078, 6876, 2006, 1996, 4274, 4743, 3074, 12133, 1006, 2005, 5619, 1010, 11645, 1010, 4380, 1010, 2413, 23568, 1010, 18786, 1010, 13884, 1007]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9mSOV7GhvdA",
        "outputId": "27cb658c-b21f-4778-c657-3ac6bf644fbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "max_len = 0\n",
        "\n",
        "for d in docs:\n",
        "    # tokenize text and add `[CLS]` and `[SEP]` tokens\n",
        "    input_ids = tokenizer.encode(d, add_special_tokens=True)\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max length: ', max_len)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1886 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3977 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (969 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2361 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (8733 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (8611 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3334 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1934 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (793 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2333 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (4684 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2756 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (9994 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (12884 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1732 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (5899 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1335 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (6808 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (711 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3995 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (870 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (13671 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (7536 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (5636 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (4778 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1153 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (5647 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (522 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (9362 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1161 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (13013 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (836 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2364 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2267 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1854 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (13013 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (4585 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1306 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (942 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (6161 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1285 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (651 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2367 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (17924 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1634 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (778 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2033 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (5039 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2913 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (4684 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (7218 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (5692 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2942 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1010 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1153 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3159 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1053 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1606 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (9362 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (5162 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (9362 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (8733 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (7259 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2501 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3019 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1159 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (899 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2671 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3792 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (7501 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (824 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1744 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (13671 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2422 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (640 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1095 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (607 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (9362 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (13013 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2227 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1983 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (4684 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (781 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (749 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1597 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (11434 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (12686 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3002 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1079 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1886 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3401 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2235 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2048 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (13013 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (9362 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (6161 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1210 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1427 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2100 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (4307 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (6559 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1217 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1096 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1166 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2546 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (667 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2914 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (719 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (708 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (9362 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3190 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1262 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (8733 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1203 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2297 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (9852 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2601 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1597 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2542 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2496 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3239 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-c0fd2cdf0401>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# tokenize text and add `[CLS]` and `[SEP]` tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m             \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1733\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1734\u001b[0m         )\n\u001b[1;32m   1735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2048\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2049\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2050\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2051\u001b[0m         )\n\u001b[1;32m   2052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0mis_split_into_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"is_pretokenized\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m         \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                     raise ValueError(\n\u001b[0;32m--> 445\u001b[0;31m                         \u001b[0;34mf\"Input {text} is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m                     )\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input nan is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgtsq-3s0jyF",
        "outputId": "5aa692b2-ba80-48b8-ad3c-a09514d77b55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "# Finishing tokenizing all docs and map tokens to thier word IDs\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for d in docs:\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        d,                      # Docs to encode.\n",
        "                        truncation=True,\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 256,           # Pad & truncate all docs\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Attention masks\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', docs[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "print('Reverse:', tokenizer.convert_ids_to_tokens(input_ids[0]))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-2b35847e5fae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                         \u001b[0mpad_to_max_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                         \u001b[0mreturn_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;31m# Attention masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                         \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'pt'\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0;31m# Return pytorch tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                    )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2048\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2049\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2050\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2051\u001b[0m         )\n\u001b[1;32m   2052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0mis_split_into_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"is_pretokenized\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m         \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                     raise ValueError(\n\u001b[0;32m--> 445\u001b[0;31m                         \u001b[0;34mf\"Input {text} is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m                     )\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input nan is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVofTi9U01pb"
      },
      "source": [
        "# Split up training & testing/validation\n",
        "\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# 80:20 split\n",
        "\n",
        "# Number of docs to include per set\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training docs'.format(train_size))\n",
        "print('{:>5,} validation docs'.format(val_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo-Gh4q78BJM"
      },
      "source": [
        "# Iterator using torch DataLoader class so that entire dataset doesn't need to be stored in memory\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# batch size can be 16 or 32\n",
        "batch_size = 32\n",
        "\n",
        "# Sample in random order when training\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  \n",
        "            sampler = RandomSampler(train_dataset), \n",
        "            batch_size = batch_size \n",
        "        )\n",
        "\n",
        "# Sample sequentially for validation\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, \n",
        "            sampler = SequentialSampler(val_dataset), \n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za2M5jHMV3LU"
      },
      "source": [
        "## Training the Classification Model w/ Sequence Classification\n",
        "  (fine-tune BERT)\n",
        "\n",
        "  [HuggingFace documentation](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BMF2gFEV0Y8"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# BertForSequenceClassification -> BERT model w/ added classification layer \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # 12-layer model, uncased vocab\n",
        "    num_labels = 5, # Number of labels \n",
        "    \"\"\"CHANGE ABOVE\"\"\"\n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False, \n",
        ")\n",
        "\n",
        "# this needs to be run on GPU\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "775rSApKXaHu"
      },
      "source": [
        "## Optimizer for our hypermarameters / Learning Rate Scheduler\n",
        "AdamW\n",
        "\n",
        "Possible hyperparamters: \n",
        "* batch size: 16, 32\n",
        "* learning rate: 5e-5, 3e-5, 2e-5\n",
        "* number of epochs: 2, 3, 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa5RhInLXfIv"
      },
      "source": [
        "# Exeprimenting w/ different parameters\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, \n",
        "                  eps = 1e-8 # epsilon prevents division by 0??\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV6rf_-BYLWS"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Training epochs should be betw 2- 4 (reduce if overfitting)\n",
        "epochs = 4\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# LR scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAdyiqxGYpg5"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk9UkHBSYsDS"
      },
      "source": [
        "import random\n",
        "\n",
        "# based on huggingface transformers `run_glue.py` script : https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_vals = []\n",
        "total_time = time.time()\n",
        "\n",
        "for epoch_i in range(0, epochs):       \n",
        "    ## TRAINING\n",
        "    print(\"\")\n",
        "    print('-------- Epoch {:} / {:} --------'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "    print(\"\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 20 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            # progress from every 20 batches\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # pytorch tensors in batch (gpu usage)\n",
        "        #   batch[0] -> input ids \n",
        "        #   batch[1] -> attention masks\n",
        "        #   batch[2] -> labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # clear gradients before forward and backward passes\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # forward pass\n",
        "        loss, logits = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # clip to prevent exploding gradients (??)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # optimizer and lr updare\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    ## VALIDATION\n",
        "    \n",
        "    print(\"\")\n",
        "    print(\"Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # pytorch tensors in batch (gpu usage)\n",
        "        #   batch[0] -> input ids \n",
        "        #   batch[1] -> attention masks\n",
        "        #   batch[2] -> labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # forward pass\n",
        "            (loss, logits) = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            \n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # move logits and labels -> CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()  \n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "    # print final validation accuracy\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)   \n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # epoch values & stats\n",
        "    training_vals.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "print(\"\")\n",
        "print(\"Training done\")\n",
        "print(\"Time to train: {:} (h:mm:ss)\".format(format_time(time.time()-total_time)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-ijNX7XD9-n"
      },
      "source": [
        "# Display metrics of training process in a dataframe\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "df_vals = pd.DataFrame(data=training_stats)\n",
        "df_vals = df_vals.set_index('epoch')\n",
        "\n",
        "df_vals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MCG2WV5y15i"
      },
      "source": [
        "input_ids_test = []\n",
        "attention_masks_test = []\n",
        "actual_labels_test=[]\n",
        "\n",
        "for i in range(500):\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sentences_test[i],                      \n",
        "                        add_special_tokens = True, \n",
        "                        max_length = 256,           \n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   \n",
        "                        return_tensors = 'pt',     \n",
        "                   )\n",
        "    \n",
        "   \n",
        "    input_ids_test.append(encoded_dict['input_ids'])\n",
        "    \n",
        "\n",
        "    attention_masks_test.append(encoded_dict['attention_mask'])\n",
        "    actual_labels_test.append(labels_test[i])\n",
        "\n",
        "# lists -> tensors\n",
        "input_ids_test = torch.cat(input_ids_test, dim=0)\n",
        "attention_masks_test = torch.cat(attention_masks_test, dim=0)\n",
        "actual_labels_test = torch.tensor(actual_labels_test)\n",
        "\n",
        "batch_size = 32  \n",
        "\n",
        "# build DataLoader\n",
        "prediction_data = TensorDataset(input_ids_test, attention_masks_test, actual_labels_test)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aspjuWhzC8h"
      },
      "source": [
        "## Testing Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giqwQoNlzHmu"
      },
      "source": [
        "print('Label predictions for {:,} test publications...'.format(len(input_ids_test)))\n",
        "model.eval()\n",
        "\n",
        "predictions, actual_labels = [], []\n",
        "\n",
        "for batch in prediction_dataloader:\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  # save memory and accelerate predictions w/o storing gradients\n",
        "    with torch.no_grad():\n",
        "      # forward pass and logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # move logits and labels -> CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  labels_ids_test = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  predictions.append(logits)\n",
        "  actual_labels.append(labels_ids_test)\n",
        "\n",
        "classification_correct = 0\n",
        "\n",
        "for i in range(len(predictions)):\n",
        "  for j in range(len(predictions[i])):\n",
        "    prediction = np.argmax(predictions[i][j])\n",
        "    print ('Prediction: ' , prediction , ', actual: ', actual_labels[i][j])\n",
        "    if prediction == actual_labels[i][j]:\n",
        "      classification_correct = classification_correct + 1\n",
        "\n",
        "print ('Classification correctly: ',  classification_correct)\n",
        "\n",
        "print ('Model accuracy from testing: {0:.2f}'.format(classification_correct / len(input_ids_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceVT2Jmoph4x"
      },
      "source": [
        "## Ignore this cell for now\n",
        "# Trying out example BERT\n",
        "\n",
        "# Single training/test example for simple sequence classification\n",
        "class InputExample(object):\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "            label: (Optional) string. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"Single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Gp0ZzW6Taop"
      },
      "source": [
        "# Initializing a BERT bert-base-uncased style configuration\n",
        "configuration = BertConfig()\n",
        "\n",
        "# Initializing a model from the bert-base-uncased style configuration\n",
        "model = BertModel(configuration)\n",
        "\n",
        "model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qr0ajbVbFL7y"
      },
      "source": [
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}